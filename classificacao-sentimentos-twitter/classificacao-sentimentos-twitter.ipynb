{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd1bf89",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-20T20:12:36.580201Z",
     "iopub.status.busy": "2025-03-20T20:12:36.579733Z",
     "iopub.status.idle": "2025-03-20T20:12:39.914967Z",
     "shell.execute_reply": "2025-03-20T20:12:39.913543Z"
    },
    "papermill": {
     "duration": 3.341061,
     "end_time": "2025-03-20T20:12:39.916716",
     "exception": true,
     "start_time": "2025-03-20T20:12:36.575655",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'twitter_training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-69a67212617e>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Carregar os datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter_training.csv'"
     ]
    }
   ],
   "source": [
    "# Utilizar  NLP para analisar dados de redes sociais (tweets, postagens, comentários) e classificar sentimentos (positivo, negativo, neutro).\n",
    "\n",
    "# Entrada: Dados extraídos de APIs como Twitter API ou arquivos locais.\n",
    "\n",
    "# Apresentar as métricas de avaliação Acurácia, F1-Score, Precisão e Recall.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Caminhos dos arquivos\n",
    "training_file = \"twitter_training.csv\"\n",
    "validation_file = \"twitter_validation.csv\"\n",
    "\n",
    "# Carregar os datasets\n",
    "training_data = pd.read_csv(training_file, header=None)\n",
    "validation_data = pd.read_csv(validation_file, header=None)\n",
    "\n",
    "# Renomear colunas\n",
    "training_data.columns = ['ID', 'Source', 'Sentiment', 'Text']\n",
    "validation_data.columns = ['ID', 'Source', 'Sentiment', 'Text']\n",
    "\n",
    "# Visualizar os primeiros dados\n",
    "print(\"Treinamento:\")\n",
    "print(training_data.tail())\n",
    "\n",
    "print(\"\\nValidação:\")\n",
    "print(validation_data.head())\n",
    "\n",
    "training_data = training_data.dropna()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribuição dos sentimentos no conjunto de treinamento\n",
    "sentiment_counts = training_data['Sentiment'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sentiment_counts.plot(kind='bar')\n",
    "plt.title('Distribuição de Sentimentos no Conjunto de Treinamento')\n",
    "plt.xlabel('Sentimentos')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Função para limpar texto\n",
    "def preprocess_text(text):\n",
    "    port_stemmer = PorterStemmer()\n",
    "    if not isinstance(text, str):  # Verificar se é string\n",
    "        text = str(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)    # Remove menções\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)    # Remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text) # Remove pontuação\n",
    "    text = text.lower()                 # Converte para minúsculas\n",
    "    #text = text.split()                 # Separa cada palavra\n",
    "    #text = [port_stemmer.stem(word) for word in text if not word in stopwords.words('english')] # Coloca as palavras em sua forma base\n",
    "    #text = ' '.join(text)               # Reune as palavras\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Aplicar a função de pré-processamento\n",
    "training_data['Cleaned_Text'] = training_data['Text'].apply(preprocess_text)\n",
    "validation_data['Cleaned_Text'] = validation_data['Text'].apply(preprocess_text)\n",
    "\n",
    "# Visualizar texto limpo\n",
    "print(training_data[['Text', 'Cleaned_Text']].head())\n",
    "\n",
    "# Comprimento dos textos antes e depois do pré-processamento\n",
    "training_data['Original_Length'] = training_data['Text'].apply(lambda x: len(str(x)))\n",
    "training_data['Cleaned_Length'] = training_data['Cleaned_Text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot([training_data['Original_Length'], training_data['Cleaned_Length']], tick_labels=['Original', 'Limpo'])\n",
    "plt.title('Comparação do Comprimento dos Textos')\n",
    "plt.ylabel('Número de Caracteres')\n",
    "plt.show()\n",
    "\n",
    "# Transformar textos em vetores numéricos\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar o vetor no conjunto de treinamento e transformar os dados\n",
    "X_train_vec = vectorizer.fit_transform(training_data['Cleaned_Text'])\n",
    "X_val_vec = vectorizer.transform(validation_data['Cleaned_Text'])\n",
    "X_train_tf = tfidf_vectorizer.fit_transform(training_data['Cleaned_Text'])\n",
    "X_val_tf = tfidf_vectorizer.transform(validation_data['Cleaned_Text'])\n",
    "\n",
    "# Rótulos\n",
    "y_train = training_data['Sentiment']\n",
    "y_val = validation_data['Sentiment']\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "model_tf = MultinomialNB()\n",
    "model_tf.fit(X_train_tf, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred = model.predict(X_val_vec)\n",
    "y_pred_tf = model_tf.predict(X_val_tf)\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='weighted')\n",
    "recall = recall_score(y_val, y_pred, average='weighted')\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\n",
    "accuracy_tf = accuracy_score(y_val, y_pred_tf)\n",
    "precision_tf = precision_score(y_val, y_pred_tf, average='weighted')\n",
    "recall_tf = recall_score(y_val, y_pred_tf, average='weighted')\n",
    "f1_tf = f1_score(y_val, y_pred_tf, average='weighted')\n",
    "\n",
    "# Exibir métricas\n",
    "print(f\"Acurácia: {(100*accuracy):.2f}%\")\n",
    "print(f\"Precisão: {(100*precision):.2f}%\")\n",
    "print(f\"Recall: {(100*recall):.2f}%\")\n",
    "print(f\"F1-Score: {(100*f1):.2f}%\\n\")\n",
    "\n",
    "print(f\"Acurácia tfidf: {(100*accuracy_tf):.2f}%\")\n",
    "print(f\"Precisão tfidf: {(100*precision_tf):.2f}%\")\n",
    "print(f\"Recall tfidf: {(100*recall_tf):.2f}%\")\n",
    "print(f\"F1-Score tfidf: {(100*f1_tf):.2f}%\")\n",
    "# Comparação de métricas\n",
    "metrics = ['Acurácia', 'Precisão', 'Recall', 'F1-Score']\n",
    "values_count = [accuracy, precision, recall, f1]\n",
    "values_tfidf = [accuracy_tf, precision_tf, recall_tf, f1_tf]\n",
    "\n",
    "x = range(len(metrics))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x, values_count, width=0.4, label='CountVectorizer', align='center')\n",
    "plt.bar([p + 0.4 for p in x], values_tfidf, width=0.4, label='TF-IDF', align='center')\n",
    "plt.xticks([p + 0.2 for p in x], metrics)\n",
    "plt.title('Comparação de Métricas entre CountVectorizer e TF-IDF')\n",
    "plt.xlabel('Métricas')\n",
    "plt.ylabel('Valores')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exibir relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(classification_report(y_val, y_pred_tf))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6925479,
     "sourceId": 11108473,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.991038,
   "end_time": "2025-03-20T20:12:40.738974",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-20T20:12:33.747936",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
